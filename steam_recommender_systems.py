# -*- coding: utf-8 -*-
"""Steam Recommender Systems.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CTOejr8UKiCTQ0GxTX6uu7K9OKVh2_RV

# Steam Recommender System - Task 2

## Introduction

As a long-time gamer and active member of the digital gaming community, I’ve always been intrigued by the systems that shape our gaming experiences behind the scenes. This project offered a particularly exciting opportunity: to build a recommender system tailored for Steam, one of the largest and most diverse online video game platforms.

The aim is to explore how Steam might suggest games to both newcomers—who are just beginning their journey—and seasoned players like myself who are constantly searching for fresh challenges and immersive worlds. To ground this project in a personal context, I’ve included a screenshot of my own Steam account. This not only reflects my hands-on familiarity with the platform but also reinforces my motivation for developing a user-centric recommendation engine.

On a technical level, this task involves building a collaborative filtering recommender system using a dataset containing user interactions such as purchases and hours played. The project follows a structured pipeline:

- Loading and preprocessing the data using PySpark.
- Exploring behavioral patterns through data analysis.
- Training a recommendation model using the ALS (Alternating Least Squares) algorithm.
- Tuning model performance with hyperparameter optimization.
- Tracking experiments using MLflow for transparency and reproducibility.

The result is an end-to-end system capable of generating personalized game recommendations—grounded in both real user behavior and practical machine learning workflows.

## Data Import and Preprocessing

We begin by loading the `steam-200k.csv` dataset into a Spark DataFrame. The dataset includes:
- `user_id`: Unique identifier for each user
- `game`: Name of the game
- `behavior`: Type of interaction (`purchase` or `play`)
- `value`: If behavior is purchase, this is 1. If play, it reflects the number of hours played.

We will clean the dataset, ensure correct data types, and generate numeric IDs for games and users required for ALS model training.
"""

from google.colab import drive
drive.mount('/content/drive')

# Install Java (already installed, but safe to include)
!apt-get install openjdk-11-jdk -y

# Download Spark from an official mirror
!wget https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz

# Extract the archive
!tar -xvzf spark-3.3.0-bin-hadoop3.tgz

# Install findspark
!pip install -q findspark

import os
import findspark

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.3.0-bin-hadoop2.7"

findspark.init()

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("SteamALS").getOrCreate()
spark

file_path = "/content/drive/MyDrive/Colab Notebooks/Steam Recommender system/steam-200k.csv"

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, monotonically_increasing_id
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

spark = SparkSession.builder \
    .appName("SteamALS") \
    .getOrCreate()

# Load the CSV
#file_path = "steam-200k.csv"

# Define schema
schema = StructType([
    StructField("user_id", StringType(), True),
    StructField("game", StringType(), True),
    StructField("behavior", StringType(), True),
    StructField("value", StringType(), True)
])

# Read data
df_raw = spark.read.csv(file_path, schema=schema, header=False)

# Convert value to float
df = df_raw.withColumn("value", col("value").cast("float"))

# Check data
df.show(5)
df.printSchema()

"""### Data Preprocessing

We will prepare the dataset for training a collaborative filtering model using ALS (Alternating Least Squares). The ALS algorithm in PySpark requires numerical `userId`, `itemId`, and `rating` columns. To do this:

1. We'll **filter out only 'play' behaviors** to use the number of hours played as implicit feedback.
2. We'll **cast `user_id` to integer** (as it's currently string).
3. We'll **generate a unique integer ID** for each game using `StringIndexer` to satisfy ALS input requirements.
"""

from pyspark.ml.feature import StringIndexer

# Filter only 'play' behavior
df_play = df.filter(df.behavior == "play")

# Convert user_id to LongType
df_play = df_play.withColumn("user_id", col("user_id").cast("long"))

# Generate integer IDs for games using StringIndexer
game_indexer = StringIndexer(inputCol="game", outputCol="game_id")
df_indexed = game_indexer.fit(df_play).transform(df_play)

# Select only required columns for ALS
df_final = df_indexed.select("user_id", "game_id", "value")

df_final.show(5)

"""The output above confirms that the dataset has been successfully loaded into a Spark DataFrame. Each row contains information about a user’s interaction with a game, including whether they purchased or played it and, if played, for how many hours.

The schema also confirms that:
- `user_id`, `game`, and `behavior` are stored as strings
- `value` has been cast correctly to a float, allowing us to use it numerically in later steps

## Exploratory Data Analysis (EDA)

In this section, we perform basic analysis to understand the data distribution. This includes:
- Number of unique users and games
- Distribution of play hours
- Most played games by total hours
These insights help us understand user engagement and inform our model decisions.

### Unique Users and Games

We begin our analysis by checking how many unique users and games exist in the dataset. This gives us a sense of the scale and diversity of interactions.
"""

from pyspark.sql.functions import countDistinct

num_users = df_final.select(countDistinct("user_id")).first()[0]
num_games = df_final.select(countDistinct("game_id")).first()[0]

print(f"Number of unique users: {num_users}")
print(f"Number of unique games: {num_games}")

"""The dataset contains **11,350 unique users** and **3,600 unique games**, indicating a wide and varied user base with diverse gaming preferences.

### Distribution of Play Hours

Next, we examine the distribution of playtime (in hours). This helps us understand typical user engagement levels and spot any outliers.
"""

df_final.describe("value").show()

"""The average playtime is approximately **48.88 hours**, but the standard deviation is quite high, indicating a wide range in user behavior. Some users have played games for over **11,000 hours**, showing the presence of heavy gamers or outliers.

### Most Played Games

We now identify the top 10 games with the highest total playtime across all users. These titles likely represent the most engaging or popular games in the dataset.
"""

from pyspark.sql.functions import col, avg, desc, sum

most_played = df_play.groupBy("game").agg(sum("value").alias("total_playtime")).orderBy(desc("total_playtime"))
most_played.show(10, truncate=False)

"""As expected, **Dota 2** leads the list by a large margin, followed by **CS:GO**, **Team Fortress 2**, and others. These games are known for their replayability and active communities, which explains the high total hours.

### Visualizing Most Played Games

To better understand user engagement, we visualized the top 10 most played games by total hours. This helps highlight which games dominate user attention on the Steam platform.
"""

import matplotlib.pyplot as plt

# Aggregate and sort data
playtime = df_play.groupBy("game").sum("value").orderBy("sum(value)", ascending=False)

# Convert to Pandas for visualization
playtime_pd = playtime.toPandas()

# Plot
plt.figure(figsize=(10, 6))
plt.barh(playtime_pd["game"][:10][::-1], playtime_pd["sum(value)"][:10][::-1])
plt.title("Top 10 Games by Total Play Time")
plt.xlabel("Total Play Time (Hours)")
plt.ylabel("Game Name")
plt.tight_layout()
plt.show()

"""The bar chart clearly shows that a small number of games account for a large portion of the total playtime, with **Dota 2** significantly ahead of the rest. Games like **CS:GO**, **Team Fortress 2**, and **Skyrim** also show high engagement levels.

These insights validate the need for a recommendation system—many users gravitate towards a few popular titles, and a recommender could help surface lesser-known but relevant games based on similar patterns of user behavior.

## Model Training and Evaluation

To build our recommender system, we used the ALS (Alternating Least Squares) algorithm from PySpark MLlib. ALS is a matrix factorization technique that decomposes the user-item interaction matrix into latent user and item factors. The algorithm alternates between optimizing user and item matrices—solving one while holding the other fixed—making the process computationally efficient.

Because our dataset involves **implicit feedback** (hours played), we enabled `implicitPrefs=True` in the ALS configuration. The model was trained using:

- `userCol="user_id"`
- `itemCol="game_id"`
- `ratingCol="value"`

We manually selected combinations of three key hyperparameters to test:

- **Rank**: Number of latent features (e.g., 10, 20)
- **RegParam**: Regularization to avoid overfitting (e.g., 0.01, 0.1)
- **MaxIter**: Number of training iterations (e.g., 5, 10)

Each model run was evaluated using Root Mean Squared Error (RMSE), and all experiments were tracked via **MLflow** to ensure reproducibility and traceability.

### Training the ALS Model

We split our dataset into training and testing sets, and then trained the ALS model using our chosen configuration. The model was trained to predict the number of hours a user might spend on a game they haven't yet played.
"""

from pyspark.ml.recommendation import ALS

(training, test) = df_final.randomSplit([0.8, 0.2], seed=42)

als = ALS(
    userCol="user_id",
    itemCol="game_id",
    ratingCol="value",
    coldStartStrategy="drop",
    implicitPrefs=True,
    nonnegative=True
)

als_model = als.fit(training)

"""###Experiment Dashboard

Below is a snapshot of the experiment run.

![Results](/files/tables/first_run.png)
![Results](/files/tables/first_run_1.png)
![Results](/files/tables/first_run_2.png)

### Model Evaluation

We used Root Mean Squared Error (RMSE) to evaluate the performance of the ALS model on the test set.
"""

from pyspark.ml.evaluation import RegressionEvaluator

predictions = als_model.transform(test)

evaluator = RegressionEvaluator(
    metricName="rmse",
    labelCol="value",
    predictionCol="prediction"
)
rmse = evaluator.evaluate(predictions)
print(f"Root-mean-square error = {rmse:.4f}")

"""### Best Model Summary

After testing multiple parameter combinations, the configuration with the lowest RMSE was selected as the final model:

- **RMSE**: 211.9099
- **Rank**: 20  
- **RegParam**: 0.01  
- **MaxIter**: 10

This model was chosen for its balance of prediction accuracy and generalization. The tuning process, supported by MLflow experiment tracking, provided insights into how small adjustments in regularization and latent factor size can impact performance.

## Hyperparameter Tuning

We now experiment with different combinations of ALS parameters to improve model accuracy:

- `rank`: Number of latent factors in the model
- `regParam`: Regularization parameter
- `alpha`: Controls confidence levels in implicit feedback

We'll evaluate each combination using RMSE to identify the best model.
"""

ranks = [10, 20]
regParams = [0.01, 0.1]
alphas = [1.0, 10.0]

best_model = None
best_rmse = float("inf")
best_params = {}

for rank in ranks:
    for reg in regParams:
        for alpha in alphas:
            als = ALS(
                userCol="user_id",
                itemCol="game_id",
                ratingCol="value",
                coldStartStrategy="drop",
                implicitPrefs=True,
                nonnegative=True,
                rank=rank,
                regParam=reg,
                alpha=alpha
            )
            model = als.fit(training)
            preds = model.transform(test)
            rmse = evaluator.evaluate(preds)
            print(f"rank={rank}, regParam={reg}, alpha={alpha} -> RMSE: {rmse:.4f}")
            if rmse < best_rmse:
                best_rmse = rmse
                best_model = model
                best_params = {"rank": rank, "regParam": reg, "alpha": alpha}

print(f"\n Best Model Params: {best_params} with RMSE: {best_rmse:.4f}")

"""### MLflow Experiment Dashboard

Below is a snapshot of the MLflow dashboard showing all experiment runs, hyperparameter combinations, and RMSE scores:

![MLflow Results](/files/tables/experiments_logs.png)

## Experiment Tracking with MLflow

To ensure reproducibility and visibility into the training process, we use MLflow to track different ALS model runs. For each combination of hyperparameters, we log the configuration and the resulting RMSE metric.
"""

!pip install mlflow

import mlflow
import mlflow.spark

mlflow.set_experiment("/Users/m.tariq11@edu.salford.ac.uk/steam_recommender")  # Replace with your path

ranks = [10, 20]
regParams = [0.01, 0.1]
alphas = [1.0, 10.0]

with mlflow.start_run(run_name="ALS_Hyperparameter_Tuning"):

    for rank in ranks:
        for reg in regParams:
            for alpha in alphas:
                als = ALS(
                    userCol="user_id",
                    itemCol="game_id",
                    ratingCol="value",
                    coldStartStrategy="drop",
                    implicitPrefs=True,
                    nonnegative=True,
                    rank=rank,
                    regParam=reg,
                    alpha=alpha
                )
                model = als.fit(training)
                predictions = model.transform(test)
                rmse = evaluator.evaluate(predictions)

                # Track run in MLflow
                with mlflow.start_run(nested=True):
                    mlflow.log_param("rank", rank)
                    mlflow.log_param("regParam", reg)
                    mlflow.log_param("alpha", alpha)
                    mlflow.log_metric("rmse", rmse)
                    mlflow.spark.log_model(model, "ALS_model")

"""### MLflow Experiment Dashboard

Below is a snapshot of the MLflow dashboard showing all experiment runs, hyperparameter combinations, and RMSE scores:

![MLflow Results](/files/tables/mflow.png)

### Best Model Selection

From the set of tested combinations, the model with the lowest Root Mean Squared Error (RMSE) was selected as the best-performing configuration:

- **RMSE**: 211.8659
- **Rank**: 20  
- **RegParam**: 0.01  
- **Alpha**: 10.0

This configuration struck the best balance between prediction accuracy and model complexity. The results also highlight how small adjustments in regularization and confidence parameters can influence performance. These insights are particularly valuable when optimizing recommender systems in real-world environments.

## Generating Recommendations

With our best-performing ALS model, we generate Top-10 game recommendations for all users in the dataset. These are personalized predictions based on collaborative filtering — the model suggests games that a user has not yet played but is likely to enjoy based on patterns of similar users.

The result is a list of recommended game IDs with predicted confidence scores for each user.
"""

# Top 10 game recommendations for all users
user_recommendations = best_model.recommendForAllUsers(10)
user_recommendations.show(5, truncate=False)

"""The output above displays a sample of the top 10 recommended games for five different users. Each entry includes a `user_id` and a list of game IDs along with predicted confidence scores (`game_id`, `rating`).

Since ALS uses collaborative filtering based on implicit feedback, these scores represent the model’s confidence that the user would engage with the recommended games — even though they haven't interacted with them yet.

To make these recommendations more interpretable, we can optionally join them with the original game titles using the mapping generated during preprocessing.

## Conclusion

In this project, we developed a collaborative filtering recommender system using implicit feedback from the Steam gaming platform dataset. We used Spark's ALS algorithm to build the model, focusing on user-game interactions based on hours played.

Our workflow included:
- Cleaning and preprocessing over 70,000 'play' records from 11,000+ users and 3,600+ games.
- Performing EDA to understand data distribution and engagement.
- Training an ALS model with various hyperparameter configurations.
- Tracking all experiments using MLflow for reproducibility and model comparison.
- Generating personalized game recommendations for each user.

The best model achieved an RMSE of **211.8659** using:
- `rank = 20`
- `regParam = 0.01`
- `alpha = 10.0`

This notebook demonstrates a complete end-to-end pipeline for building and tuning a recommender system in a scalable big data environment using Databricks.
"""

